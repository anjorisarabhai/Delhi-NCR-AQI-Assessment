{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03528b7",
   "metadata": {},
   "source": [
    "# MLflow Discrepancy Prediction Pipeline\n",
    "\n",
    "This notebook implements an end-to-end ML pipeline to predict the discrepancy between ground and satellite PM2.5 measurements.\n",
    "\n",
    "## Overview\n",
    "- **Target**: `target_diff = PM2.5_ground - PM2.5_satellite * scaling_factor`\n",
    "- **Models**: Linear Regression, Ridge, Lasso, Random Forest, Gradient Boosting, XGBoost\n",
    "- **Tracking**: MLflow for experiment tracking and model versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd902e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp313-cp313-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from matplotlib) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from seaborn) (2.3.3)\n",
      "Collecting mlflow-skinny==3.6.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.6.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting mlflow-tracing==3.6.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow)\n",
      "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow) (45.0.5)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting huey<3,>=2.5.0 (from mlflow)\n",
      "  Downloading huey-2.5.4-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pyarrow<23,>=4.0.0 (from mlflow)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting scikit-learn<2 (from mlflow)\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting scipy<2 (from mlflow)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (8.2.1)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading databricks_sdk-0.73.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading fastapi-0.121.3-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (8.7.0)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading protobuf-6.33.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (2.11.7)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (1.1.0)\n",
      "Collecting pyyaml<7,>=5.1 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (2.32.4)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (4.12.2)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\robot\\miniconda3\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.6.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from cryptography<47,>=43.0.0->mlflow) (1.17.1)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (311)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Collecting starlette<0.51.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting jinja2>=3.1.2 (from Flask<4->mlflow)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe>=2.1.1 (from Flask<4->mlflow)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow) (3.23.0)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\robot\\miniconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (2025.11.12)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.6.0->mlflow)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\robot\\miniconda3\\lib\\site-packages (from cffi>=1.14->cryptography<47,>=43.0.0->mlflow) (2.21)\n",
      "Downloading matplotlib-3.10.7-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/8.1 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 1.8/8.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.4/8.1 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.9/8.1 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.1/8.1 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.1 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.2/8.1 MB 2.6 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.5/8.1 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.1 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading mlflow-3.6.0-py3-none-any.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.1/8.9 MB 10.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.9 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.0/8.9 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading mlflow_skinny-3.6.0-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 9.7 MB/s eta 0:00:00\n",
      "Downloading mlflow_tracing-3.6.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 12.3 MB/s eta 0:00:00\n",
      "Downloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Downloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading databricks_sdk-0.73.0-py3-none-any.whl (753 kB)\n",
      "   ---------------------------------------- 0.0/753.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 753.9/753.9 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading fastapi-0.121.3-py3-none-any.whl (109 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading huey-2.5.4-py3-none-any.whl (76 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-6.33.1-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.3/28.0 MB 5.4 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/28.0 MB 6.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 4.2/28.0 MB 6.7 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 5.5/28.0 MB 6.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 7.3/28.0 MB 6.8 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 9.2/28.0 MB 7.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 11.0/28.0 MB 7.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 13.9/28.0 MB 8.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 16.3/28.0 MB 8.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 19.4/28.0 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 21.8/28.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.7/28.0 MB 9.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.8/28.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 9.4 MB/s eta 0:00:00\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-win_amd64.whl (154 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/38.5 MB 8.3 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.5/38.5 MB 11.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 6.8/38.5 MB 11.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 9.4/38.5 MB 11.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 11.3/38.5 MB 11.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 13.6/38.5 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 16.3/38.5 MB 11.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.4/38.5 MB 11.6 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 22.0/38.5 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 24.9/38.5 MB 12.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 27.3/38.5 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.1/38.5 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.5/38.5 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.6/38.5 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.9/38.5 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.5 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 11.5 MB/s eta 0:00:00\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.1/2.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Downloading xgboost-3.1.2-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.6/72.0 MB 7.9 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 4.5/72.0 MB 10.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 6.6/72.0 MB 11.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 9.4/72.0 MB 11.2 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 12.1/72.0 MB 11.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 14.9/72.0 MB 11.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 12.4 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 21.0/72.0 MB 12.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 24.1/72.0 MB 12.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 26.7/72.0 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 28.6/72.0 MB 12.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 30.7/72.0 MB 12.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 33.0/72.0 MB 12.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 36.2/72.0 MB 12.3 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 38.5/72.0 MB 12.2 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 41.2/72.0 MB 12.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 43.3/72.0 MB 12.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 45.4/72.0 MB 12.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 47.4/72.0 MB 12.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 49.8/72.0 MB 11.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 52.2/72.0 MB 12.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 54.3/72.0 MB 11.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 56.6/72.0 MB 11.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 60.0/72.0 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 62.1/72.0 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 65.0/72.0 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 67.4/72.0 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  70.3/72.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 11.9 MB/s eta 0:00:00\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 2.1/2.3 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.4-cp313-cp313-win_amd64.whl (299 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.8/7.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 4.2/7.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.3/7.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: huey, waitress, threadpoolctl, sqlparse, sniffio, smmap, scipy, pyyaml, pyparsing, pyasn1, pyarrow, protobuf, pillow, markupsafe, kiwisolver, joblib, itsdangerous, h11, greenlet, graphql-core, fonttools, cycler, contourpy, cloudpickle, cachetools, blinker, annotated-doc, xgboost, werkzeug, uvicorn, sqlalchemy, scikit-learn, rsa, pyasn1-modules, opentelemetry-proto, opentelemetry-api, matplotlib, Mako, jinja2, graphql-relay, gitdb, docker, anyio, starlette, seaborn, opentelemetry-semantic-conventions, graphene, google-auth, gitpython, Flask, alembic, opentelemetry-sdk, Flask-CORS, fastapi, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
      "\n",
      "   ----------------------------------------  0/58 [huey]\n",
      "   -- -------------------------------------  3/58 [sqlparse]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ---- -----------------------------------  6/58 [scipy]\n",
      "   ----- ----------------------------------  8/58 [pyparsing]\n",
      "   ------ ---------------------------------  9/58 [pyasn1]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------ --------------------------------- 10/58 [pyarrow]\n",
      "   ------- -------------------------------- 11/58 [protobuf]\n",
      "   ------- -------------------------------- 11/58 [protobuf]\n",
      "   -------- ------------------------------- 12/58 [pillow]\n",
      "   -------- ------------------------------- 12/58 [pillow]\n",
      "   -------- ------------------------------- 12/58 [pillow]\n",
      "   ---------- ----------------------------- 15/58 [joblib]\n",
      "   ---------- ----------------------------- 15/58 [joblib]\n",
      "   ---------- ----------------------------- 15/58 [joblib]\n",
      "   ------------ --------------------------- 18/58 [greenlet]\n",
      "   ------------- -------------------------- 19/58 [graphql-core]\n",
      "   ------------- -------------------------- 19/58 [graphql-core]\n",
      "   ------------- -------------------------- 19/58 [graphql-core]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   ------------- -------------------------- 20/58 [fonttools]\n",
      "   --------------- ------------------------ 23/58 [cloudpickle]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------ --------------------- 27/58 [xgboost]\n",
      "   ------------------- -------------------- 28/58 [werkzeug]\n",
      "   -------------------- ------------------- 29/58 [uvicorn]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   -------------------- ------------------- 30/58 [sqlalchemy]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   --------------------- ------------------ 31/58 [scikit-learn]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 32/58 [rsa]\n",
      "   ---------------------- ----------------- 33/58 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 33/58 [pyasn1-modules]\n",
      "   ---------------------- ----------------- 33/58 [pyasn1-modules]\n",
      "   ----------------------- ---------------- 34/58 [opentelemetry-proto]\n",
      "   ------------------------ --------------- 35/58 [opentelemetry-api]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------ --------------- 36/58 [matplotlib]\n",
      "   ------------------------- -------------- 37/58 [Mako]\n",
      "   -------------------------- ------------- 38/58 [jinja2]\n",
      "   --------------------------- ------------ 40/58 [gitdb]\n",
      "   ---------------------------- ----------- 41/58 [docker]\n",
      "   ---------------------------- ----------- 42/58 [anyio]\n",
      "   ----------------------------- ---------- 43/58 [starlette]\n",
      "   ------------------------------ --------- 44/58 [seaborn]\n",
      "   ------------------------------ --------- 44/58 [seaborn]\n",
      "   ------------------------------ --------- 44/58 [seaborn]\n",
      "   ------------------------- ------- 45/58 [opentelemetry-semantic-conventions]\n",
      "   ------------------------- ------- 45/58 [opentelemetry-semantic-conventions]\n",
      "   ------------------------------- -------- 46/58 [graphene]\n",
      "   ------------------------------- -------- 46/58 [graphene]\n",
      "   -------------------------------- ------- 47/58 [google-auth]\n",
      "   -------------------------------- ------- 47/58 [google-auth]\n",
      "   --------------------------------- ------ 48/58 [gitpython]\n",
      "   --------------------------------- ------ 49/58 [Flask]\n",
      "   ---------------------------------- ----- 50/58 [alembic]\n",
      "   ---------------------------------- ----- 50/58 [alembic]\n",
      "   ---------------------------------- ----- 50/58 [alembic]\n",
      "   ----------------------------------- ---- 51/58 [opentelemetry-sdk]\n",
      "   ----------------------------------- ---- 52/58 [Flask-CORS]\n",
      "   ------------------------------------ --- 53/58 [fastapi]\n",
      "   ------------------------------------- -- 54/58 [databricks-sdk]\n",
      "   ------------------------------------- -- 54/58 [databricks-sdk]\n",
      "   ------------------------------------- -- 54/58 [databricks-sdk]\n",
      "   ------------------------------------- -- 54/58 [databricks-sdk]\n",
      "   ------------------------------------- -- 54/58 [databricks-sdk]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   ------------------------------------- -- 55/58 [mlflow-tracing]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   -------------------------------------- - 56/58 [mlflow-skinny]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------  57/58 [mlflow]\n",
      "   ---------------------------------------- 58/58 [mlflow]\n",
      "\n",
      "Successfully installed Flask-3.1.2 Flask-CORS-6.0.1 Mako-1.3.10 alembic-1.17.2 annotated-doc-0.0.4 anyio-4.11.0 blinker-1.9.0 cachetools-6.2.2 cloudpickle-3.1.2 contourpy-1.3.3 cycler-0.12.1 databricks-sdk-0.73.0 docker-7.1.0 fastapi-0.121.3 fonttools-4.60.1 gitdb-4.0.12 gitpython-3.1.45 google-auth-2.43.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.2.4 h11-0.16.0 huey-2.5.4 itsdangerous-2.2.0 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 markupsafe-3.0.3 matplotlib-3.10.7 mlflow-3.6.0 mlflow-skinny-3.6.0 mlflow-tracing-3.6.0 opentelemetry-api-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 pillow-12.0.0 protobuf-6.33.1 pyarrow-22.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.5 pyyaml-6.0.3 rsa-4.9.1 scikit-learn-1.7.2 scipy-1.16.3 seaborn-0.13.2 smmap-5.0.2 sniffio-1.3.1 sqlalchemy-2.0.44 sqlparse-0.5.3 starlette-0.50.0 threadpoolctl-3.6.0 uvicorn-0.38.0 waitress-3.0.2 werkzeug-3.1.3 xgboost-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib seaborn mlflow xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f647e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 17:49:42 INFO mlflow.tracking.fluent: Experiment with name 'PM2.5_Discrepancy_Prediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: C:\\Users\\robot\\Desktop\\Delhi-NCR-AQI-Assessment\n",
      "Data path: C:\\Users\\robot\\Desktop\\Delhi-NCR-AQI-Assessment\\notebooks\\model-training\\cleaned_aqi_merged_dataset.csv\n",
      "Models directory: C:\\Users\\robot\\Desktop\\Delhi-NCR-AQI-Assessment\\models\n",
      "MLflow tracking URI: C:\\Users\\robot\\Desktop\\Delhi-NCR-AQI-Assessment\\mlruns\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "\n",
    "# XGBoost (optional)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Skipping XGBoost model.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "DATA_PATH = NOTEBOOK_DIR / \"cleaned_aqi_merged_dataset.csv\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# MLflow setup\n",
    "MLFLOW_TRACKING_URI = PROJECT_ROOT / \"mlruns\"\n",
    "os.makedirs(MLFLOW_TRACKING_URI, exist_ok=True)\n",
    "# Convert to file URI format for Windows compatibility\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI.as_uri())\n",
    "mlflow.set_experiment(\"PM2.5_Discrepancy_Prediction\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0767a",
   "metadata": {},
   "source": [
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bffd575f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (16936, 34)\n",
      "\n",
      "Columns: ['date', 'NO2_satellite', 'SO2_satellite', 'CO_satellite', 'O3_satellite', 'Aerosol_Index_satellite', 'location', 'PM2.5_ground', 'PM10_ground', 'NO2_ground', 'SO2_ground', 'CO_ground', 'O3_ground', 'lat', 'lon', 'notes', 'distance_to_major_road', 'total_road_length_m', 'major_road_length_m', 'pct_green', 'pct_industrial', 'pct_residential', 'building_density', 'avg_building_area_m2', 'median_building_area_m2', 'building_count', 'major_road_fraction', 'month', 'day_of_week', 'season', 'NO2_ratio', 'SO2_ratio', 'CO_ratio', 'O3_ratio']\n",
      "\n",
      "First few rows:\n",
      "         date  NO2_satellite  SO2_satellite  CO_satellite  O3_satellite  \\\n",
      "0  2020-01-01       0.000191      -0.000433      0.048550      0.164568   \n",
      "1  2020-01-02       0.000191      -0.000433      0.048550      0.164568   \n",
      "2  2020-01-03       0.000143       0.000546      0.041712      0.139937   \n",
      "3  2020-01-04       0.000143      -0.000315      0.044633      0.137429   \n",
      "4  2020-01-05       0.000143      -0.000531      0.045669      0.128115   \n",
      "\n",
      "   Aerosol_Index_satellite            location  PM2.5_ground  PM10_ground  \\\n",
      "0                -1.098919  Anand Vihar, Delhi        367.38       449.58   \n",
      "1                -1.098919  Anand Vihar, Delhi        360.34       463.33   \n",
      "2                -1.405038  Anand Vihar, Delhi        394.86       500.24   \n",
      "3                -1.210190  Anand Vihar, Delhi        204.94       359.77   \n",
      "4                -0.864201  Anand Vihar, Delhi        186.40       313.40   \n",
      "\n",
      "   NO2_ground  ...  median_building_area_m2  building_count  \\\n",
      "0       54.76  ...               354.930527           511.0   \n",
      "1       61.22  ...               354.930527           511.0   \n",
      "2       46.96  ...               354.930527           511.0   \n",
      "3       90.79  ...               354.930527           511.0   \n",
      "4       69.24  ...               354.930527           511.0   \n",
      "\n",
      "   major_road_fraction  month  day_of_week  season      NO2_ratio  \\\n",
      "0             0.186949      1            2  Winter  286676.055011   \n",
      "1             0.186949      1            3  Winter  320495.034473   \n",
      "2             0.186949      1            4  Winter  328311.252491   \n",
      "3             0.186949      1            5  Winter  635214.933393   \n",
      "4             0.186949      1            6  Winter  484806.049573   \n",
      "\n",
      "      SO2_ratio   CO_ratio    O3_ratio  \n",
      "0 -24850.228872  75.179980  364.955240  \n",
      "1 -49192.367562  85.066663  387.863686  \n",
      "2  40533.012783  70.243097  471.211298  \n",
      "3 -15756.636212  20.612365  459.656254  \n",
      "4 -20912.816165  42.917242  490.887442  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "\n",
      "Missing values per column:\n",
      "No missing values found.\n",
      "\n",
      "Basic statistics:\n",
      "       NO2_satellite  SO2_satellite  CO_satellite  O3_satellite  \\\n",
      "count   16936.000000   16936.000000  16936.000000  16936.000000   \n",
      "mean        0.000143       0.000121      0.041496      0.126609   \n",
      "std         0.000113       0.000506      0.006591      0.006832   \n",
      "min         0.000015      -0.000998      0.023917      0.105325   \n",
      "25%         0.000081      -0.000167      0.037052      0.122285   \n",
      "50%         0.000109       0.000053      0.040150      0.126065   \n",
      "75%         0.000164       0.000327      0.044650      0.130713   \n",
      "max         0.001703       0.006777      0.087687      0.166354   \n",
      "\n",
      "       Aerosol_Index_satellite  PM2.5_ground   PM10_ground    NO2_ground  \\\n",
      "count             16936.000000  16936.000000  16936.000000  16936.000000   \n",
      "mean                 -0.211805     96.293899    238.809977     40.859577   \n",
      "std                   0.911143     76.544658    159.097628     33.410156   \n",
      "min                  -5.423089      1.000000      1.000000      0.010000   \n",
      "25%                  -0.888522     43.670000    115.717500     17.315852   \n",
      "50%                  -0.232872     72.207195    193.310000     31.810000   \n",
      "75%                   0.387326    124.420000    318.725000     56.032500   \n",
      "max                   4.218647    725.860000    859.340000    317.960000   \n",
      "\n",
      "         SO2_ground     CO_ground  ...  avg_building_area_m2  \\\n",
      "count  16936.000000  16936.000000  ...          16936.000000   \n",
      "mean      14.094494      6.068010  ...            805.884142   \n",
      "std       10.676977     15.246478  ...            417.056271   \n",
      "min        0.220000      0.000000  ...            218.319406   \n",
      "25%        7.020000      0.750000  ...            553.340217   \n",
      "50%       11.890000      1.170000  ...            716.254919   \n",
      "75%       18.630000      2.120000  ...            961.265597   \n",
      "max      167.860000    308.600000  ...           1502.014514   \n",
      "\n",
      "       median_building_area_m2  building_count  major_road_fraction  \\\n",
      "count             16936.000000    16936.000000         16936.000000   \n",
      "mean                416.861057     1015.000000             0.210922   \n",
      "std                 160.204199     1035.638851             0.060349   \n",
      "min                 169.965864      100.000000             0.100468   \n",
      "25%                 337.112704      430.750000             0.190997   \n",
      "50%                 377.157044      813.000000             0.202153   \n",
      "75%                 468.803481     1029.750000             0.228131   \n",
      "max                 704.276851     3604.000000             0.326433   \n",
      "\n",
      "              month   day_of_week     NO2_ratio     SO2_ratio      CO_ratio  \\\n",
      "count  16936.000000  16936.000000  1.693600e+04  1.693600e+04  16936.000000   \n",
      "mean       6.355220      3.000000  3.400735e+05  2.705331e+04    145.259402   \n",
      "std        3.389131      1.998878  2.748037e+05  6.890147e+06    366.141740   \n",
      "min        1.000000      0.000000  1.103753e+02 -5.323077e+08      0.000000   \n",
      "25%        3.000000      1.000000  1.512280e+05 -4.831109e+04     18.592826   \n",
      "50%        6.000000      3.000000  2.777441e+05  8.975637e+03     28.533553   \n",
      "75%        9.000000      5.000000  4.520916e+05  5.089672e+04     49.408684   \n",
      "max       12.000000      6.000000  3.871157e+06  5.723404e+08   8151.923106   \n",
      "\n",
      "           O3_ratio  \n",
      "count  16936.000000  \n",
      "mean     230.594646  \n",
      "std      181.456288  \n",
      "min        0.000000  \n",
      "25%       93.558627  \n",
      "50%      209.322866  \n",
      "75%      319.936760  \n",
      "max     2749.559918  \n",
      "\n",
      "[8 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6480df",
   "metadata": {},
   "source": [
    "## 2. Prepare Data & Create Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1055ec3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using Aerosol_Index_satellite as proxy for satellite PM2.5\n",
      "   (Satellite data does not have direct PM2.5 measurements - PM2.5/PM10 are ground-exclusive)\n",
      "\n",
      "Ground PM2.5 column: PM2.5_ground\n",
      "Satellite PM2.5 column: Aerosol_Index_satellite\n",
      "Scaling factor column: None (using default=1)\n",
      "Using scaling factor 50.0 for Aerosol_Index to PM2.5 conversion\n",
      "\n",
      "Target variable statistics:\n",
      "count    16936.000000\n",
      "mean       106.884137\n",
      "std         78.310584\n",
      "min       -153.024503\n",
      "25%         57.002818\n",
      "50%         92.474619\n",
      "75%        138.477370\n",
      "max        643.586179\n",
      "Name: target_diff, dtype: float64\n",
      "\n",
      "Filtering unrealistic values...\n",
      "Removed 0 rows with unrealistic values\n",
      "Final dataset shape: (16936, 35)\n"
     ]
    }
   ],
   "source": [
    "# Identify PM2.5 columns (flexible naming)\n",
    "ground_col = None\n",
    "satellite_col = None\n",
    "scaling_factor_col = None\n",
    "\n",
    "# Try to find ground PM2.5 column\n",
    "# PM2.5_ground\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'pm2.5_ground' in col.lower():\n",
    "        ground_col = col\n",
    "        break\n",
    "\n",
    "# Use Aerosol_Index_satellite as proxy for satellite PM2.5\n",
    "# Note: Satellite data does NOT have PM2.5/PM10 - these are ground-exclusive metrics\n",
    "# Satellite data only has: NO2, SO2, CO, O3, and Aerosol_Index\n",
    "if 'Aerosol_Index_satellite' in df.columns:\n",
    "    satellite_col = 'Aerosol_Index_satellite'\n",
    "    print(\"  Using Aerosol_Index_satellite as proxy for satellite PM2.5\")\n",
    "    print(\"   (Satellite data does not have direct PM2.5 measurements - PM2.5/PM10 are ground-exclusive)\")\n",
    "else:\n",
    "    raise ValueError(\" ERROR: Aerosol_Index_satellite column not found!\")\n",
    "\n",
    "# Check for scaling factor\n",
    "for col in df.columns:\n",
    "    if 'scaling_factor' in col.lower():\n",
    "        scaling_factor_col = col\n",
    "        break\n",
    "\n",
    "print(f\"\\nGround PM2.5 column: {ground_col}\")\n",
    "print(f\"Satellite PM2.5 column: {satellite_col}\")\n",
    "print(f\"Scaling factor column: {scaling_factor_col if scaling_factor_col else 'None (using default=1)'}\")\n",
    "\n",
    "# Create target variable\n",
    "if scaling_factor_col:\n",
    "    df[\"target_diff\"] = df[ground_col] - (df[satellite_col] * df[scaling_factor_col])\n",
    "else:\n",
    "    # If using Aerosol_Index, we need a reasonable scaling (AOD typically ranges -2 to 2)\n",
    "    # For PM2.5, a rough conversion: PM2.5  AOD * 50-100 (this is approximate)\n",
    "    if satellite_col == 'Aerosol_Index_satellite':\n",
    "        # Use a scaling factor to convert AOD to approximate PM2.5\n",
    "        # This is a rough estimate - adjust based on domain knowledge\n",
    "        scaling_factor = 50.0  # Approximate conversion factor\n",
    "        df[\"target_diff\"] = df[ground_col] - (df[satellite_col] * scaling_factor)\n",
    "        print(f\"Using scaling factor {scaling_factor} for Aerosol_Index to PM2.5 conversion\")\n",
    "    else:\n",
    "        df[\"target_diff\"] = df[ground_col] - df[satellite_col]\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(df[\"target_diff\"].describe())\n",
    "\n",
    "# Drop unrealistic values\n",
    "print(\"\\nFiltering unrealistic values...\")\n",
    "initial_shape = df.shape[0]\n",
    "df = df[(df[ground_col] >= -10) & (df[ground_col] <= 1000)]\n",
    "if satellite_col != 'Aerosol_Index_satellite':\n",
    "    df = df[(df[satellite_col] >= -10) & (df[satellite_col] <= 1000)]\n",
    "df = df[df[\"target_diff\"].notna()]\n",
    "final_shape = df.shape[0]\n",
    "print(f\"Removed {initial_shape - final_shape} rows with unrealistic values\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04992fd3",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete pipeline with preprocessor and best model\n",
    "import joblib\n",
    "\n",
    "# Get the best model class\n",
    "model_classes = {\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'Ridge': lambda: Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': lambda: Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': lambda: RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': lambda: GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    model_classes['XGBoost'] = lambda: xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a fresh instance of the best model\n",
    "fresh_best_model = model_classes[best_model_name]()\n",
    "\n",
    "# Create a complete pipeline (preprocessor + model)\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', fresh_best_model)\n",
    "])\n",
    "\n",
    "# Retrain on full training data\n",
    "print(f\"Retraining {best_model_name} on full training set...\")\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n Pipeline trained successfully\")\n",
    "print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "print(f\"  Test R: {test_r2:.4f}\")\n",
    "\n",
    "# Save the complete pipeline to disk\n",
    "best_model_path = MODELS_DIR / \"best_model_pipeline.pkl\"\n",
    "joblib.dump(best_pipeline, best_model_path)\n",
    "print(f\"\\n Best model pipeline saved to: {best_model_path}\")\n",
    "\n",
    "# Also save metadata for easy loading\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'feature_columns': feature_cols,\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'numerical_columns': numerical_cols,\n",
    "    'ground_col': ground_col,\n",
    "    'satellite_col': satellite_col,\n",
    "    'scaling_factor': 50.0 if satellite_col == 'Aerosol_Index_satellite' else 1.0,\n",
    "    'test_rmse': float(test_rmse),\n",
    "    'test_mae': float(test_mae),\n",
    "    'test_r2': float(test_r2)\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = MODELS_DIR / \"model_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\" Model metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Save using MLflow as well\n",
    "with mlflow.start_run(run_name=f\"{best_model_name}_final\"):\n",
    "    mlflow.log_param(\"model_name\", f\"{best_model_name}_final\")\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    mlflow.sklearn.log_model(best_pipeline, \"best_model\")\n",
    "    print(f\" Model also saved to MLflow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087deb61",
   "metadata": {},
   "source": [
    "## 9. Load and Use Saved Model (Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fabcfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load and use the saved model in your code\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths (works in both notebook and script)\n",
    "try:\n",
    "    # In notebook\n",
    "    MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "except NameError:\n",
    "    # In standalone script\n",
    "    PROJECT_ROOT = Path(__file__).parent.parent\n",
    "    MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_PATH = MODELS_DIR / \"best_model_pipeline.pkl\"\n",
    "METADATA_PATH = MODELS_DIR / \"model_metadata.json\"\n",
    "\n",
    "# Load the model pipeline\n",
    "print(\"Loading saved model...\")\n",
    "loaded_pipeline = joblib.load(MODEL_PATH)\n",
    "print(\" Model loaded successfully!\")\n",
    "\n",
    "# Load metadata\n",
    "with open(METADATA_PATH, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "print(f\"\\nModel Info:\")\n",
    "print(f\"  Model Type: {metadata['model_name']}\")\n",
    "print(f\"  Test RMSE: {metadata['test_rmse']:.4f}\")\n",
    "print(f\"  Test R: {metadata['test_r2']:.4f}\")\n",
    "\n",
    "# Example: Make predictions on new data\n",
    "def predict_discrepancy(features_df):\n",
    "    \"\"\"\n",
    "    Predict discrepancy between ground and satellite PM2.5.\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with all required feature columns\n",
    "        \n",
    "    Returns:\n",
    "        Array of predicted discrepancies\n",
    "    \"\"\"\n",
    "    return loaded_pipeline.predict(features_df)\n",
    "\n",
    "\n",
    "def predict_corrected_pm25(features_df, satellite_value):\n",
    "    \"\"\"\n",
    "    Predict corrected PM2.5 using satellite value and predicted discrepancy.\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame with all required feature columns\n",
    "        satellite_value: Satellite AOD value (Aerosol_Index_satellite)\n",
    "        \n",
    "    Returns:\n",
    "        Array of corrected PM2.5 values\n",
    "    \"\"\"\n",
    "    predicted_diff = predict_discrepancy(features_df)\n",
    "    scaling_factor = metadata['scaling_factor']\n",
    "    corrected_pm25 = (satellite_value * scaling_factor) + predicted_diff\n",
    "    return corrected_pm25\n",
    "\n",
    "\n",
    "# Example usage with sample data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example: Making predictions on new data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create sample feature data (you would replace this with your actual data)\n",
    "sample_data = pd.DataFrame({\n",
    "    'NO2_satellite': [0.000191],\n",
    "    'SO2_satellite': [-0.000433],\n",
    "    'CO_satellite': [0.048550],\n",
    "    'O3_satellite': [0.164568],\n",
    "    'location': ['Anand Vihar, Delhi'],\n",
    "    'PM10_ground': [449.58],\n",
    "    'NO2_ground': [54.76],\n",
    "    'SO2_ground': [10.5],\n",
    "    'CO_ground': [2.1],\n",
    "    'O3_ground': [60.0],\n",
    "    'lat': [28.65],\n",
    "    'lon': [77.31],\n",
    "    'distance_to_major_road': [150.0],\n",
    "    'total_road_length_m': [5000.0],\n",
    "    'major_road_length_m': [1000.0],\n",
    "    'pct_green': [15.5],\n",
    "    'pct_industrial': [20.0],\n",
    "    'pct_residential': [40.0],\n",
    "    'building_density': [0.8],\n",
    "    'avg_building_area_m2': [800.0],\n",
    "    'median_building_area_m2': [350.0],\n",
    "    'building_count': [500.0],\n",
    "    'major_road_fraction': [0.19],\n",
    "    'month': [1],\n",
    "    'day_of_week': [2],\n",
    "    'season': ['Winter'],\n",
    "    'NO2_ratio': [286676.0],\n",
    "    'SO2_ratio': [-24850.0],\n",
    "    'CO_ratio': [75.0],\n",
    "    'O3_ratio': [365.0]\n",
    "})\n",
    "\n",
    "# Ensure all required columns are present\n",
    "required_cols = metadata['feature_columns']\n",
    "for col in required_cols:\n",
    "    if col not in sample_data.columns:\n",
    "        print(f\"Warning: Missing column {col}, using default value\")\n",
    "        sample_data[col] = 0  # or use appropriate default\n",
    "\n",
    "# Reorder columns to match training data\n",
    "sample_data = sample_data[required_cols]\n",
    "\n",
    "# Make predictions\n",
    "satellite_aod = -1.098919  # Example AOD value\n",
    "predicted_diff = predict_discrepancy(sample_data)\n",
    "corrected_pm25 = predict_corrected_pm25(sample_data, satellite_aod)\n",
    "\n",
    "print(f\"\\nPredicted Discrepancy: {predicted_diff[0]:.2f}\")\n",
    "print(f\"Satellite AOD: {satellite_aod}\")\n",
    "print(f\"Corrected PM2.5: {corrected_pm25[0]:.2f}\")\n",
    "print(f\"\\n Prediction complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"To use in your own code:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# In your Python script:\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models/best_model_pipeline.pkl\")\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Prepare your data as a DataFrame with all feature columns\n",
    "# Then simply call:\n",
    "predictions = model.predict(your_dataframe)\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba6b362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns (30):\n",
      "['NO2_satellite', 'SO2_satellite', 'CO_satellite', 'O3_satellite', 'location', 'PM10_ground', 'NO2_ground', 'SO2_ground', 'CO_ground', 'O3_ground', 'lat', 'lon', 'distance_to_major_road', 'total_road_length_m', 'major_road_length_m', 'pct_green', 'pct_industrial', 'pct_residential', 'building_density', 'avg_building_area_m2', 'median_building_area_m2', 'building_count', 'major_road_fraction', 'month', 'day_of_week', 'season', 'NO2_ratio', 'SO2_ratio', 'CO_ratio', 'O3_ratio']\n",
      "\n",
      "Categorical columns (2): ['location', 'season']\n",
      "Numerical columns (28): ['NO2_satellite', 'SO2_satellite', 'CO_satellite', 'O3_satellite', 'PM10_ground', 'NO2_ground', 'SO2_ground', 'CO_ground', 'O3_ground', 'lat', 'lon', 'distance_to_major_road', 'total_road_length_m', 'major_road_length_m', 'pct_green', 'pct_industrial', 'pct_residential', 'building_density', 'avg_building_area_m2', 'median_building_area_m2', 'building_count', 'major_road_fraction', 'month', 'day_of_week', 'NO2_ratio', 'SO2_ratio', 'CO_ratio', 'O3_ratio']\n",
      "\n",
      "Missing values in features:\n",
      "No missing values found.\n",
      "\n",
      "Preprocessing pipeline created.\n"
     ]
    }
   ],
   "source": [
    "# Identify feature columns\n",
    "# Exclude target, date, location (we'll encode location), and other non-feature columns\n",
    "exclude_cols = ['date', 'target_diff', ground_col, satellite_col, 'notes']\n",
    "if scaling_factor_col:\n",
    "    exclude_cols.append(scaling_factor_col)\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"target_diff\"].copy()\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values in features:\")\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    # Fill numerical with median, categorical with mode\n",
    "    for col in numerical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    for col in categorical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "    print(\"Missing values filled.\")\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d4c63",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550b403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (13548, 30)\n",
      "Test set shape: (3388, 30)\n",
      "Training target shape: (13548,)\n",
      "Test target shape: (3388,)\n",
      "\n",
      "Processed training set shape: (13548, 38)\n",
      "Processed test set shape: (3388, 38)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "\n",
    "# Fit preprocessor on training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nProcessed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test set shape: {X_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7259b38",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab18ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training with MLflow tracking...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable MLflow autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"Starting model training with MLflow tracking...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffc75e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:11:15 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training LinearRegression...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:11:20 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:11:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/21 18:11:24 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LinearRegression trained successfully\n",
      "  Test MAE: 44.6432\n",
      "  Test RMSE: 57.9994\n",
      "  Test R: 0.4885\n",
      "\n",
      "============================================================\n",
      "Training Ridge...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:11:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:11:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/21 18:11:33 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ridge trained successfully\n",
      "  Test MAE: 44.6425\n",
      "  Test RMSE: 57.9994\n",
      "  Test R: 0.4885\n",
      "\n",
      "============================================================\n",
      "Training Lasso...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:11:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:11:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/21 18:11:42 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lasso trained successfully\n",
      "  Test MAE: 44.5995\n",
      "  Test RMSE: 58.2992\n",
      "  Test R: 0.4832\n",
      "\n",
      "============================================================\n",
      "Training RandomForest...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:11:54 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:11:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/11/21 18:11:58 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RandomForest trained successfully\n",
      "  Test MAE: 29.1926\n",
      "  Test RMSE: 39.3684\n",
      "  Test R: 0.7643\n",
      "\n",
      "============================================================\n",
      "Training GradientBoosting...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:12:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:12:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GradientBoosting trained successfully\n",
      "  Test MAE: 34.3096\n",
      "  Test RMSE: 45.2439\n",
      "  Test R: 0.6887\n",
      "\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/21 18:12:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/21 18:12:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " XGBoost trained successfully\n",
      "  Test MAE: 30.3293\n",
      "  Test RMSE: 40.2996\n",
      "  Test R: 0.7530\n",
      "\n",
      "============================================================\n",
      "All models trained successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log model name\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train_processed)\n",
    "        y_test_pred = model.predict(X_test_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        \n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_mae\", train_mae)\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Log top 10 features\n",
    "            top_features = importance_df.head(10)\n",
    "            for idx, row in top_features.iterrows():\n",
    "                # Sanitize feature name for MLflow (replace invalid characters)\n",
    "                # MLflow allows: alphanumerics, underscores, dashes, periods, spaces, and slashes\n",
    "                feature_name = str(row['feature'])\n",
    "                # Replace commas and other invalid chars with underscores\n",
    "                sanitized_name = feature_name.replace(',', '_').replace('(', '_').replace(')', '_')\n",
    "                sanitized_name = sanitized_name.replace('[', '_').replace(']', '_').replace('{', '_').replace('}', '_')\n",
    "                sanitized_name = sanitized_name.replace(':', '_').replace(';', '_').replace('=', '_')\n",
    "                # Replace multiple underscores with single underscore\n",
    "                while '__' in sanitized_name:\n",
    "                    sanitized_name = sanitized_name.replace('__', '_')\n",
    "                mlflow.log_metric(f\"feature_importance_{sanitized_name}\", row['importance'])\n",
    "        \n",
    "        # Create and save plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Residual plot\n",
    "        residuals = y_test - y_test_pred\n",
    "        axes[0].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "        axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[0].set_xlabel('Predicted Values')\n",
    "        axes[0].set_ylabel('Residuals')\n",
    "        axes[0].set_title(f'{model_name} - Residual Plot')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Predicted vs Actual\n",
    "        axes[1].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "        min_val = min(y_test.min(), y_test_pred.min())\n",
    "        max_val = max(y_test.max(), y_test_pred.max())\n",
    "        axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        axes[1].set_xlabel('Actual Values')\n",
    "        axes[1].set_ylabel('Predicted Values')\n",
    "        axes[1].set_title(f'{model_name} - Predicted vs Actual')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plots\n",
    "        plot_path = f\"plots_{model_name}.png\"\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'train_mae': train_mae,\n",
    "            'train_rmse': train_rmse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2\n",
    "        })\n",
    "        \n",
    "        print(f\" {model_name} trained successfully\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"  Test R: {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe394fb",
   "metadata": {},
   "source": [
    "## 6. Model Comparison & Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1c4f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Summary:\n",
      "================================================================================\n",
      "           model  train_mae  train_rmse  train_r2  test_mae  test_rmse  test_r2\n",
      "    RandomForest  11.026185   14.973852  0.962759 29.192581  39.368394 0.764320\n",
      "    RandomForest  11.026185   14.973852  0.962759 29.192581  39.368394 0.764320\n",
      "         XGBoost  17.777633   23.455856  0.908619 30.329279  40.299589 0.753039\n",
      "GradientBoosting  33.976017   43.797367  0.681397 34.309580  45.243949 0.688722\n",
      "           Ridge  43.955502   56.693876  0.466141 44.642529  57.999395 0.488467\n",
      "           Ridge  43.955502   56.693876  0.466141 44.642529  57.999395 0.488467\n",
      "LinearRegression  43.956353   56.693872  0.466141 44.643166  57.999447 0.488466\n",
      "LinearRegression  43.956353   56.693872  0.466141 44.643166  57.999447 0.488466\n",
      "           Lasso  43.876452   57.074207  0.458954 44.599535  58.299171 0.483165\n",
      "           Lasso  43.876452   57.074207  0.458954 44.599535  58.299171 0.483165\n",
      "================================================================================\n",
      "\n",
      " Best Model: RandomForest\n",
      "   Test RMSE: 39.3684\n",
      "   Test MAE: 29.1926\n",
      "   Test R: 0.7643\n",
      "\n",
      " Results saved to: C:\\Users\\robot\\Desktop\\Delhi-NCR-AQI-Assessment\\model_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('test_rmse')\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best model (lowest RMSE)\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\n Best Model: {best_model_name}\")\n",
    "print(f\"   Test RMSE: {results_df.iloc[0]['test_rmse']:.4f}\")\n",
    "print(f\"   Test MAE: {results_df.iloc[0]['test_mae']:.4f}\")\n",
    "print(f\"   Test R: {results_df.iloc[0]['test_r2']:.4f}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_csv_path = PROJECT_ROOT / \"model_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\n Results saved to: {results_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8472d",
   "metadata": {},
   "source": [
    "## 7. Save Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68709506",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Get the best model class\u001b[39;00m\n\u001b[32m      6\u001b[39m model_classes = {\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLinearRegression\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mLinearRegression\u001b[49m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRidge\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: Ridge(alpha=\u001b[32m1.0\u001b[39m, random_state=\u001b[32m42\u001b[39m),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLasso\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: Lasso(alpha=\u001b[32m1.0\u001b[39m, random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: RandomForestRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m),\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mGradientBoosting\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m: GradientBoostingRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m),\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m XGBOOST_AVAILABLE:\n\u001b[32m     15\u001b[39m     model_classes[\u001b[33m'\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m: xgb.XGBRegressor(n_estimators=\u001b[32m100\u001b[39m, random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Save all trained models as complete pipelines (preprocessor + model)\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Get the model classes\n",
    "model_classes = {\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'Ridge': lambda: Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': lambda: Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': lambda: RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': lambda: GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    model_classes['XGBoost'] = lambda: xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get all model names that were trained\n",
    "all_model_names = list(model_classes.keys())\n",
    "\n",
    "print(f\"Saving {len(all_model_names)} models as complete pipelines...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Store all saved models info\n",
    "saved_models_info = {}\n",
    "all_models_metadata = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'numerical_columns': numerical_cols,\n",
    "    'ground_col': ground_col,\n",
    "    'satellite_col': satellite_col,\n",
    "    'scaling_factor': 50.0 if satellite_col == 'Aerosol_Index_satellite' else 1.0,\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "# Save each model as a complete pipeline\n",
    "for model_name in all_model_names:\n",
    "    print(f\"\\nProcessing {model_name}...\")\n",
    "    \n",
    "    # Create a fresh instance of the model\n",
    "    fresh_model = model_classes[model_name]()\n",
    "    \n",
    "    # Create a complete pipeline (preprocessor + model)\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', fresh_model)\n",
    "    ])\n",
    "    \n",
    "    # Retrain on full training data\n",
    "    print(f\"  Retraining {model_name} on full training set...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_test_pred = model_pipeline.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Save the pipeline to disk using joblib\n",
    "    model_filename = f\"{model_name.lower()}_pipeline.pkl\"\n",
    "    model_path = MODELS_DIR / model_filename\n",
    "    joblib.dump(model_pipeline, model_path)\n",
    "    \n",
    "    # Store model info\n",
    "    saved_models_info[model_name] = {\n",
    "        'filename': model_filename,\n",
    "        'path': str(model_path),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'test_mae': float(test_mae),\n",
    "        'test_r2': float(test_r2)\n",
    "    }\n",
    "    \n",
    "    all_models_metadata['models'][model_name] = {\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'test_mae': float(test_mae),\n",
    "        'test_r2': float(test_r2),\n",
    "        'filename': model_filename\n",
    "    }\n",
    "    \n",
    "    print(f\"   {model_name} saved to: {model_path}\")\n",
    "    print(f\"    Test RMSE: {test_rmse:.4f}, Test MAE: {test_mae:.4f}, Test R: {test_r2:.4f}\")\n",
    "\n",
    "# Save comprehensive metadata for all models\n",
    "metadata_path = MODELS_DIR / \"all_models_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(all_models_metadata, f, indent=2)\n",
    "print(f\"\\n All models metadata saved to: {metadata_path}\")\n",
    "\n",
    "# Also save a summary with best model info\n",
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_model_info': saved_models_info[best_model_name],\n",
    "    'all_models': saved_models_info,\n",
    "    'feature_columns': feature_cols,\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'numerical_columns': numerical_cols,\n",
    "    'ground_col': ground_col,\n",
    "    'satellite_col': satellite_col,\n",
    "    'scaling_factor': 50.0 if satellite_col == 'Aerosol_Index_satellite' else 1.0\n",
    "}\n",
    "\n",
    "summary_path = MODELS_DIR / \"models_summary.json\"\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\" Models summary saved to: {summary_path}\")\n",
    "\n",
    "# Create a symlink/copy of the best model for convenience\n",
    "best_model_path = MODELS_DIR / \"best_model_pipeline.pkl\"\n",
    "best_model_source = MODELS_DIR / saved_models_info[best_model_name]['filename']\n",
    "import shutil\n",
    "shutil.copy(best_model_source, best_model_path)\n",
    "print(f\" Best model ({best_model_name}) also saved as: {best_model_path}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models saved successfully!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nSaved models:\")\n",
    "for model_name, info in saved_models_info.items():\n",
    "    marker = \"  BEST\" if model_name == best_model_name else \"\"\n",
    "    print(f\"  - {model_name}: {info['filename']} (RMSE: {info['test_rmse']:.4f}){marker}\")\n",
    "\n",
    "print(f\"\\nTo load a model:\")\n",
    "print(f\"  import joblib\")\n",
    "print(f\"  model = joblib.load('models/{{model_name}}_pipeline.pkl')\")\n",
    "print(f\"  predictions = model.predict(your_dataframe)\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caaf96",
   "metadata": {},
   "source": [
    "## 8. Prediction Function (Optional - for making predictions on new data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80139800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict discrepancy and corrected PM2.5\n",
    "def predict_discrepancy(model_pipeline, features_df):\n",
    "    \"\"\"\n",
    "    Predict the discrepancy between ground and satellite PM2.5.\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: Trained model pipeline (preprocessor + model)\n",
    "        features_df: DataFrame with feature columns\n",
    "        \n",
    "    Returns:\n",
    "        Array of predicted discrepancies\n",
    "    \"\"\"\n",
    "    return model_pipeline.predict(features_df)\n",
    "\n",
    "\n",
    "def predict_corrected_pm25(model_pipeline, features_df, satellite_value, scaling_factor=1.0):\n",
    "    \"\"\"\n",
    "    Predict corrected PM2.5 using satellite value and predicted discrepancy.\n",
    "    \n",
    "    Formula: corrected_pm25 = satellite_value * scaling_factor + predicted_difference\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: Trained model pipeline\n",
    "        features_df: DataFrame with feature columns\n",
    "        satellite_value: Satellite PM2.5 or AOD value\n",
    "        scaling_factor: Scaling factor for satellite value (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Array of corrected PM2.5 values\n",
    "    \"\"\"\n",
    "    predicted_diff = predict_discrepancy(model_pipeline, features_df)\n",
    "    corrected_pm25 = (satellite_value * scaling_factor) + predicted_diff\n",
    "    return corrected_pm25\n",
    "\n",
    "\n",
    "# Example: Load saved model and make predictions\n",
    "# Uncomment and modify as needed:\n",
    "\"\"\"\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(best_model_path)\n",
    "\n",
    "# Example: Predict on new data\n",
    "# new_data = pd.DataFrame({\n",
    "#     'NO2_satellite': [0.0002],\n",
    "#     'SO2_satellite': [-0.0004],\n",
    "#     'CO_satellite': [0.05],\n",
    "#     'O3_satellite': [0.16],\n",
    "#     'Aerosol_Index_satellite': [-1.0],\n",
    "#     'location': ['Anand Vihar, Delhi'],\n",
    "#     'month': [1],\n",
    "#     'day_of_week': [2],\n",
    "#     'season': ['Winter'],\n",
    "#     # ... add all required features\n",
    "# })\n",
    "\n",
    "# predicted_diff = predict_discrepancy(loaded_model, new_data)\n",
    "# corrected_pm25 = predict_corrected_pm25(loaded_model, new_data, -1.0, scaling_factor=50.0)\n",
    "\n",
    "# print(f\"Predicted discrepancy: {predicted_diff[0]:.2f}\")\n",
    "# print(f\"Corrected PM2.5: {corrected_pm25[0]:.2f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prediction functions defined. Uncomment the example code above to use them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh instance of the best model (since previous one was trained on processed data)\n",
    "model_classes = {\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'Ridge': lambda: Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': lambda: Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': lambda: RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': lambda: GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    model_classes['XGBoost'] = lambda: xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a fresh model instance\n",
    "fresh_best_model = model_classes[best_model_name]()\n",
    "\n",
    "# Create a complete pipeline (preprocessor + model)\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', fresh_best_model)\n",
    "])\n",
    "\n",
    "# Retrain on full training data\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model using MLflow\n",
    "with mlflow.start_run(run_name=f\"{best_model_name}_final\"):\n",
    "    mlflow.log_param(\"model_name\", f\"{best_model_name}_final\")\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_test_pred = best_pipeline.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    # Save the complete pipeline\n",
    "    mlflow.sklearn.log_model(best_pipeline, \"best_model\")\n",
    "    \n",
    "    # Also save to local directory\n",
    "    import joblib\n",
    "    best_model_path = MODELS_DIR / \"best_model.pkl\"\n",
    "    joblib.dump(best_pipeline, best_model_path)\n",
    "    print(f\" Best model saved to: {best_model_path}\")\n",
    "\n",
    "print(f\"\\n Final model saved using MLflow\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "print(f\"  Test R: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
