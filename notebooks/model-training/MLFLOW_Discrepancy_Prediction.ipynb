{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03528b7",
   "metadata": {},
   "source": [
    "# MLflow Discrepancy Prediction Pipeline\n",
    "\n",
    "This notebook implements an end-to-end ML pipeline to predict the discrepancy between ground and satellite PM2.5 measurements.\n",
    "\n",
    "## Overview\n",
    "- **Target**: `target_diff = PM2.5_ground - PM2.5_satellite * scaling_factor`\n",
    "- **Models**: Linear Regression, Ridge, Lasso, Random Forest, Gradient Boosting, XGBoost\n",
    "- **Tracking**: MLflow for experiment tracking and model versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f647e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# MLflow imports\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "\n",
    "# XGBoost (optional)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available. Skipping XGBoost model.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up paths\n",
    "NOTEBOOK_DIR = Path().resolve()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
    "DATA_PATH = NOTEBOOK_DIR / \"cleaned_aqi_merged_dataset.csv\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# MLflow setup\n",
    "MLFLOW_TRACKING_URI = PROJECT_ROOT / \"mlruns\"\n",
    "os.makedirs(MLFLOW_TRACKING_URI, exist_ok=True)\n",
    "mlflow.set_tracking_uri(str(MLFLOW_TRACKING_URI))\n",
    "mlflow.set_experiment(\"PM2.5_Discrepancy_Prediction\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"MLflow tracking URI: {MLFLOW_TRACKING_URI}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0767a",
   "metadata": {},
   "source": [
    "## 1. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6480df",
   "metadata": {},
   "source": [
    "## 2. Prepare Data & Create Target Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify PM2.5 columns (flexible naming)\n",
    "ground_col = None\n",
    "satellite_col = None\n",
    "scaling_factor_col = None\n",
    "\n",
    "# Try to find ground PM2.5 column\n",
    "for col in df.columns:\n",
    "    if 'pm25_ground' in col.lower() or 'pm2.5_ground' in col.lower():\n",
    "        ground_col = col\n",
    "        break\n",
    "\n",
    "# Try to find satellite PM2.5 column\n",
    "for col in df.columns:\n",
    "    if 'pm25_satellite' in col.lower() or 'pm2.5_satellite' in col.lower():\n",
    "        satellite_col = col\n",
    "        break\n",
    "\n",
    "# If satellite PM2.5 not found, use Aerosol_Index as proxy\n",
    "if satellite_col is None:\n",
    "    if 'Aerosol_Index_satellite' in df.columns:\n",
    "        satellite_col = 'Aerosol_Index_satellite'\n",
    "        print(\"‚ö†Ô∏è  PM2.5_satellite column not found. Using Aerosol_Index_satellite as proxy.\")\n",
    "    else:\n",
    "        print(\"‚ùå ERROR: No satellite PM2.5 or Aerosol_Index column found!\")\n",
    "        raise ValueError(\"Cannot find satellite PM2.5 data\")\n",
    "\n",
    "# Check for scaling factor\n",
    "for col in df.columns:\n",
    "    if 'scaling_factor' in col.lower():\n",
    "        scaling_factor_col = col\n",
    "        break\n",
    "\n",
    "print(f\"\\nGround PM2.5 column: {ground_col}\")\n",
    "print(f\"Satellite PM2.5 column: {satellite_col}\")\n",
    "print(f\"Scaling factor column: {scaling_factor_col if scaling_factor_col else 'None (using default=1)'}\")\n",
    "\n",
    "# Create target variable\n",
    "if scaling_factor_col:\n",
    "    df[\"target_diff\"] = df[ground_col] - (df[satellite_col] * df[scaling_factor_col])\n",
    "else:\n",
    "    # If using Aerosol_Index, we need a reasonable scaling (AOD typically ranges -2 to 2)\n",
    "    # For PM2.5, a rough conversion: PM2.5 ‚âà AOD * 50-100 (this is approximate)\n",
    "    if satellite_col == 'Aerosol_Index_satellite':\n",
    "        # Use a scaling factor to convert AOD to approximate PM2.5\n",
    "        # This is a rough estimate - adjust based on domain knowledge\n",
    "        scaling_factor = 50.0  # Approximate conversion factor\n",
    "        df[\"target_diff\"] = df[ground_col] - (df[satellite_col] * scaling_factor)\n",
    "        print(f\"Using scaling factor {scaling_factor} for Aerosol_Index to PM2.5 conversion\")\n",
    "    else:\n",
    "        df[\"target_diff\"] = df[ground_col] - df[satellite_col]\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(df[\"target_diff\"].describe())\n",
    "\n",
    "# Drop unrealistic values\n",
    "print(\"\\nFiltering unrealistic values...\")\n",
    "initial_shape = df.shape[0]\n",
    "df = df[(df[ground_col] >= -10) & (df[ground_col] <= 1000)]\n",
    "if satellite_col != 'Aerosol_Index_satellite':\n",
    "    df = df[(df[satellite_col] >= -10) & (df[satellite_col] <= 1000)]\n",
    "df = df[df[\"target_diff\"].notna()]\n",
    "final_shape = df.shape[0]\n",
    "print(f\"Removed {initial_shape - final_shape} rows with unrealistic values\")\n",
    "print(f\"Final dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04992fd3",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns\n",
    "# Exclude target, date, location (we'll encode location), and other non-feature columns\n",
    "exclude_cols = ['date', 'target_diff', ground_col, satellite_col, 'notes']\n",
    "if scaling_factor_col:\n",
    "    exclude_cols.append(scaling_factor_col)\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "X = df[feature_cols].copy()\n",
    "y = df[\"target_diff\"].copy()\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(f\"\\nMissing values in features:\")\n",
    "missing_counts = X.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "    # Fill numerical with median, categorical with mode\n",
    "    for col in numerical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    for col in categorical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown', inplace=True)\n",
    "    print(\"Missing values filled.\")\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing pipeline created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d4c63",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test target shape: {y_test.shape}\")\n",
    "\n",
    "# Fit preprocessor on training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\nProcessed training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test set shape: {X_test_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7259b38",
   "metadata": {},
   "source": [
    "## 5. Model Training with MLflow Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18ffe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable MLflow autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    models['XGBoost'] = xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"Starting model training with MLflow tracking...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc75e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log model name\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_processed, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_train_pred = model.predict(X_train_processed)\n",
    "        y_test_pred = model.predict(X_test_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "        train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "        train_r2 = r2_score(y_train, y_train_pred)\n",
    "        \n",
    "        test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "        test_r2 = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"train_mae\", train_mae)\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse)\n",
    "        mlflow.log_metric(\"train_r2\", train_r2)\n",
    "        mlflow.log_metric(\"test_mae\", test_mae)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "        mlflow.log_metric(\"test_r2\", test_r2)\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Log top 10 features\n",
    "            top_features = importance_df.head(10)\n",
    "            for idx, row in top_features.iterrows():\n",
    "                mlflow.log_metric(f\"feature_importance_{row['feature']}\", row['importance'])\n",
    "        \n",
    "        # Create and save plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Residual plot\n",
    "        residuals = y_test - y_test_pred\n",
    "        axes[0].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "        axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[0].set_xlabel('Predicted Values')\n",
    "        axes[0].set_ylabel('Residuals')\n",
    "        axes[0].set_title(f'{model_name} - Residual Plot')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Predicted vs Actual\n",
    "        axes[1].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "        min_val = min(y_test.min(), y_test_pred.min())\n",
    "        max_val = max(y_test.max(), y_test_pred.max())\n",
    "        axes[1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        axes[1].set_xlabel('Actual Values')\n",
    "        axes[1].set_ylabel('Predicted Values')\n",
    "        axes[1].set_title(f'{model_name} - Predicted vs Actual')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plots\n",
    "        plot_path = f\"plots_{model_name}.png\"\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(plot_path)\n",
    "        plt.close()\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'train_mae': train_mae,\n",
    "            'train_rmse': train_rmse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_mae': test_mae,\n",
    "            'test_rmse': test_rmse,\n",
    "            'test_r2': test_r2\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì {model_name} trained successfully\")\n",
    "        print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"  Test R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All models trained successfully!\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe394fb",
   "metadata": {},
   "source": [
    "## 6. Model Comparison & Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('test_rmse')\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best model (lowest RMSE)\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Test RMSE: {results_df.iloc[0]['test_rmse']:.4f}\")\n",
    "print(f\"   Test MAE: {results_df.iloc[0]['test_mae']:.4f}\")\n",
    "print(f\"   Test R¬≤: {results_df.iloc[0]['test_r2']:.4f}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_csv_path = PROJECT_ROOT / \"model_results.csv\"\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\n‚úì Results saved to: {results_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8472d",
   "metadata": {},
   "source": [
    "## 7. Save Best Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3caaf96",
   "metadata": {},
   "source": [
    "## 8. Prediction Function (Optional - for making predictions on new data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80139800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict discrepancy and corrected PM2.5\n",
    "def predict_discrepancy(model_pipeline, features_df):\n",
    "    \"\"\"\n",
    "    Predict the discrepancy between ground and satellite PM2.5.\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: Trained model pipeline (preprocessor + model)\n",
    "        features_df: DataFrame with feature columns\n",
    "        \n",
    "    Returns:\n",
    "        Array of predicted discrepancies\n",
    "    \"\"\"\n",
    "    return model_pipeline.predict(features_df)\n",
    "\n",
    "\n",
    "def predict_corrected_pm25(model_pipeline, features_df, satellite_value, scaling_factor=1.0):\n",
    "    \"\"\"\n",
    "    Predict corrected PM2.5 using satellite value and predicted discrepancy.\n",
    "    \n",
    "    Formula: corrected_pm25 = satellite_value * scaling_factor + predicted_difference\n",
    "    \n",
    "    Args:\n",
    "        model_pipeline: Trained model pipeline\n",
    "        features_df: DataFrame with feature columns\n",
    "        satellite_value: Satellite PM2.5 or AOD value\n",
    "        scaling_factor: Scaling factor for satellite value (default: 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        Array of corrected PM2.5 values\n",
    "    \"\"\"\n",
    "    predicted_diff = predict_discrepancy(model_pipeline, features_df)\n",
    "    corrected_pm25 = (satellite_value * scaling_factor) + predicted_diff\n",
    "    return corrected_pm25\n",
    "\n",
    "\n",
    "# Example: Load saved model and make predictions\n",
    "# Uncomment and modify as needed:\n",
    "\"\"\"\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(best_model_path)\n",
    "\n",
    "# Example: Predict on new data\n",
    "# new_data = pd.DataFrame({\n",
    "#     'NO2_satellite': [0.0002],\n",
    "#     'SO2_satellite': [-0.0004],\n",
    "#     'CO_satellite': [0.05],\n",
    "#     'O3_satellite': [0.16],\n",
    "#     'Aerosol_Index_satellite': [-1.0],\n",
    "#     'location': ['Anand Vihar, Delhi'],\n",
    "#     'month': [1],\n",
    "#     'day_of_week': [2],\n",
    "#     'season': ['Winter'],\n",
    "#     # ... add all required features\n",
    "# })\n",
    "\n",
    "# predicted_diff = predict_discrepancy(loaded_model, new_data)\n",
    "# corrected_pm25 = predict_corrected_pm25(loaded_model, new_data, -1.0, scaling_factor=50.0)\n",
    "\n",
    "# print(f\"Predicted discrepancy: {predicted_diff[0]:.2f}\")\n",
    "# print(f\"Corrected PM2.5: {corrected_pm25[0]:.2f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prediction functions defined. Uncomment the example code above to use them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh instance of the best model (since previous one was trained on processed data)\n",
    "model_classes = {\n",
    "    'LinearRegression': LinearRegression,\n",
    "    'Ridge': lambda: Ridge(alpha=1.0, random_state=42),\n",
    "    'Lasso': lambda: Lasso(alpha=1.0, random_state=42),\n",
    "    'RandomForest': lambda: RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': lambda: GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    model_classes['XGBoost'] = lambda: xgb.XGBRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create a fresh model instance\n",
    "fresh_best_model = model_classes[best_model_name]()\n",
    "\n",
    "# Create a complete pipeline (preprocessor + model)\n",
    "best_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', fresh_best_model)\n",
    "])\n",
    "\n",
    "# Retrain on full training data\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save the best model using MLflow\n",
    "with mlflow.start_run(run_name=f\"{best_model_name}_final\"):\n",
    "    mlflow.log_param(\"model_name\", f\"{best_model_name}_final\")\n",
    "    mlflow.log_param(\"best_model\", best_model_name)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_test_pred = best_pipeline.predict(X_test)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    mlflow.log_metric(\"test_mae\", test_mae)\n",
    "    mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "    mlflow.log_metric(\"test_r2\", test_r2)\n",
    "    \n",
    "    # Save the complete pipeline\n",
    "    mlflow.sklearn.log_model(best_pipeline, \"best_model\")\n",
    "    \n",
    "    # Also save to local directory\n",
    "    import joblib\n",
    "    best_model_path = MODELS_DIR / \"best_model.pkl\"\n",
    "    joblib.dump(best_pipeline, best_model_path)\n",
    "    print(f\"‚úì Best model saved to: {best_model_path}\")\n",
    "\n",
    "print(f\"\\n‚úì Final model saved using MLflow\")\n",
    "print(f\"  Model: {best_model_name}\")\n",
    "print(f\"  Test RMSE: {test_rmse:.4f}\")\n",
    "print(f\"  Test MAE: {test_mae:.4f}\")\n",
    "print(f\"  Test R¬≤: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac9c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
